[æ ¹ç›®å½•](../../CLAUDE.md) > **llm**

# llm æ¨¡å—æ–‡æ¡£

## æ¨¡å—èŒè´£

llm æ¨¡å—è´Ÿè´£å°è£…å’Œç®¡ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº¤äº’ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£ä¾›å…¶ä»–æ¨¡å—è°ƒç”¨ã€‚æ”¯æŒå¤šç§AIæ¨¡å‹æœåŠ¡ï¼Œå®ç°æ™ºèƒ½åˆ†æåŠŸèƒ½ï¼Œå¹¶è®°å½•ä½¿ç”¨æƒ…å†µä»¥ä¾¿æˆæœ¬æ§åˆ¶ã€‚

## å…¥å£ä¸å¯åŠ¨

### ä¸»è¦å…¥å£æ–‡ä»¶
- **`openai_client.py`**ï¼šOpenAI APIå¢å¼ºå°è£…å®¢æˆ·ç«¯
- **`usage_logger.py`**ï¼šTokenä½¿ç”¨è®°å½•å’Œç»Ÿè®¡
- **`__init__.py`**ï¼šæ¨¡å—åˆå§‹åŒ–å’Œå¯¼å‡º

### å¿«é€Ÿä½¿ç”¨ç¤ºä¾‹

```python
from llm.openai_client import OpenAIClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨ä»é…ç½®è¯»å–APIå¯†é’¥ï¼‰
client = OpenAIClient()

# å‘é€èŠå¤©è¯·æ±‚
response = client.chat_completion(
    messages=[{"role": "user", "content": "åˆ†æè¿™æ”¯è‚¡ç¥¨"}],
    model="deepseek-chat",
    temperature=0.7
)
```

## å¯¹å¤–æ¥å£

### OpenAIClient ç±»

#### æ ¸å¿ƒæ–¹æ³•

```python
class OpenAIClient:
    def __init__(self,
                 api_key: Optional[str] = None,
                 usage_logger: Optional[UsageLogger] = None)

    def chat_completion(self,
                       messages: List[Dict[str, str]],
                       model: str = None,
                       temperature: float = 0.7,
                       max_tokens: int = None,
                       stream: bool = False) -> str
    """å‘é€èŠå¤©å®Œæˆè¯·æ±‚"""

    def async_chat_completion(self,
                             messages: List[Dict[str, str]],
                             model: str = None,
                             temperature: float = 0.7) -> str
    """å¼‚æ­¥èŠå¤©å®Œæˆè¯·æ±‚"""

    def get_model_info(self, model: str) -> Dict[str, Any]
    """è·å–æ¨¡å‹ä¿¡æ¯"""

    def estimate_tokens(self, text: str) -> int
    """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
```

#### é«˜çº§åŠŸèƒ½

```python
# å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚
client.chat_completion_with_retry(
    messages,
    max_retries=3,
    backoff_factor=2
)

# æµå¼å“åº”
for chunk in client.stream_completion(messages):
    print(chunk, end='')

# æ‰¹é‡å¤„ç†
responses = client.batch_completion(
    message_list,
    max_concurrent=5
)
```

### UsageLogger ç±»

```python
class UsageLogger:
    def log_request(self,
                    model: str,
                    prompt_tokens: int,
                    completion_tokens: int,
                    cost: float)
    """è®°å½•APIä½¿ç”¨æƒ…å†µ"""

    def get_daily_usage(self, date: str) -> Dict[str, Any]
    """è·å–æŒ‡å®šæ—¥æœŸçš„ä½¿ç”¨ç»Ÿè®¡"""

    def get_model_usage(self, model: str) -> Dict[str, Any]
    """è·å–æŒ‡å®šæ¨¡å‹çš„ä½¿ç”¨ç»Ÿè®¡"""

    def export_usage_report(self,
                           start_date: str,
                           end_date: str) -> pd.DataFrame
    """å¯¼å‡ºä½¿ç”¨æŠ¥å‘Š"""
```

## å…³é”®ä¾èµ–ä¸é…ç½®

### å¤–éƒ¨ä¾èµ–
- **openai**: OpenAIå®˜æ–¹Python SDK
- **tiktoken**: Tokenè®¡ç®—å·¥å…·
- **pandas**: æ•°æ®å¤„ç†
- **loguru**: æ—¥å¿—è®°å½•

### é…ç½®ç®¡ç†

```toml
[LLM_OPENAI]
API_KEY = "sk-xxx"
BASE_URL = ""  # å¯é€‰ï¼Œç”¨äºå…¼å®¹å…¶ä»–æœåŠ¡
TIMEOUT = 60
MAX_RETRIES = 3

[LLM_LOGGING]
USAGE_LOG_FILE = "logs/openai_usage.csv"
ENABLE_LOGGING = true
LOG_LEVEL = "INFO"

[LLM_CACHE]
ENABLE_CACHE = false
CACHE_TTL = 3600
```

### æ”¯æŒçš„æ¨¡å‹

#### OpenAI å®˜æ–¹æ¨¡å‹
- `gpt-4o`: æœ€æ–°çš„GPT-4æ¨¡å‹
- `gpt-4o-mini`: è½»é‡ç‰ˆGPT-4
- `gpt-3.5-turbo`: ç»å…¸GPT-3.5

#### DeepSeek ç³»åˆ—
- `deepseek-chat`: é€šç”¨å¯¹è¯æ¨¡å‹
- `deepseek-reasoner`: æ¨ç†ä¸“ç”¨æ¨¡å‹

#### é˜¿é‡Œç™¾ç‚¼ç³»åˆ—
- `qwen-plus`: å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
- `qwen-max`: æœ€é«˜æ€§èƒ½
- `qwen-turbo`: å¿«é€Ÿå“åº”

## æ•°æ®æ¨¡å‹

### APIRequest è¯·æ±‚è®°å½•
```python
@dataclass
class APIRequest:
    timestamp: str
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    cost: float
    response_time: float
    success: bool
``### ModelConfig æ¨¡å‹é…ç½®
```python
@dataclass
class ModelConfig:
    model_id: str
    max_tokens: int
    temperature_range: Tuple[float, float]
    cost_per_1k_tokens: Dict[str, float]  # prompt/completion
    supports_streaming: bool
    supports_functions: bool
```

## æµ‹è¯•ä¸è´¨é‡

### æµ‹è¯•è¦†ç›–
- âœ… APIè¿æ¥æµ‹è¯•
- âœ… Tokenè®¡ç®—æµ‹è¯•
- âœ… é”™è¯¯å¤„ç†æµ‹è¯•
- â³ æ€§èƒ½å‹åŠ›æµ‹è¯•å¾…å®æ–½

### è´¨é‡ä¿è¯
1. **é‡è¯•æœºåˆ¶**ï¼šè‡ªåŠ¨å¤„ç†ç½‘ç»œé”™è¯¯å’ŒAPIé™æµ
2. **è¶…æ—¶æ§åˆ¶**ï¼šé˜²æ­¢é•¿æ—¶é—´ç­‰å¾…
3. **æ—¥å¿—è®°å½•**ï¼šå®Œæ•´çš„è¯·æ±‚å’Œå“åº”æ—¥å¿—
4. **æˆæœ¬ç›‘æ§**ï¼šå®æ—¶è·Ÿè¸ªAPIä½¿ç”¨æˆæœ¬

## ä½¿ç”¨æœ€ä½³å®è·µ

### 1. æˆæœ¬ä¼˜åŒ–
```python
# é€‰æ‹©åˆé€‚çš„æ¨¡å‹
model = "deepseek-chat"  # ç»æµå®ç”¨
model = "deepseek-reasoner"  # å¤æ‚åˆ†æ

# æ§åˆ¶è¾“å‡ºé•¿åº¦
response = client.chat_completion(
    messages,
    max_tokens=1000  # é™åˆ¶è¾“å‡ºé•¿åº¦
)

# ä½¿ç”¨ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
client.chat_completion_cached(messages, ttl=3600)
```

### 2. é”™è¯¯å¤„ç†
```python
try:
    response = client.chat_completion(messages)
except OpenAIError as e:
    logger.error(f"APIè¯·æ±‚å¤±è´¥: {e}")
    # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæˆ–è¿”å›é»˜è®¤ç»“æœ
```

### 3. å¼‚æ­¥å¤„ç†
```python
import asyncio

async def analyze_stocks(stock_list):
    tasks = [
        client.async_chat_completion(messages)
        for messages in stock_list
    ]
    results = await asyncio.gather(*tasks)
    return results
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„æ¨¡å‹æä¾›å•†ï¼Ÿ
A: ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ `BASE_URL` å’Œ `API_KEY`ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é€‚é…ã€‚

### Q: å¦‚ä½•æ§åˆ¶APIè°ƒç”¨æˆæœ¬ï¼Ÿ
A: ä½¿ç”¨ `UsageLogger` ç›‘æ§ä½¿ç”¨æƒ…å†µï¼Œé€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œé™åˆ¶è¾“å‡ºé•¿åº¦ã€‚

### Q: å¦‚ä½•å¤„ç†å¤§æ–‡æœ¬è¾“å…¥ï¼Ÿ
A: ä½¿ç”¨æ–‡æœ¬åˆ†å—å¤„ç†ï¼Œæˆ–é€‰æ‹©æ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„æ¨¡å‹ï¼ˆå¦‚ `qwen-max-longcontext`ï¼‰ã€‚

### Q: å¦‚ä½•å®ç°è‡ªå®šä¹‰çš„promptæ¨¡æ¿ï¼Ÿ
A: åœ¨å…¶ä»–æ¨¡å—ï¼ˆå¦‚ `stock/analysis_prompts.py`ï¼‰ä¸­å®šä¹‰ï¼Œé€šè¿‡å‚æ•°ä¼ é€’ç»™LLMå®¢æˆ·ç«¯ã€‚

## ç›¸å…³æ–‡ä»¶æ¸…å•

```
llm/
â”œâ”€â”€ __init__.py          # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ openai_client.py     # OpenAIå®¢æˆ·ç«¯å°è£…
â””â”€â”€ usage_logger.py      # ä½¿ç”¨ç»Ÿè®¡è®°å½•
```

## å˜æ›´è®°å½•

### 2025-12-07 22:48:54
- âœ¨ åˆ›å»ºæ¨¡å—æ–‡æ¡£
- ğŸ“ å®šä¹‰LLMæ¥å£å’Œä½¿ç”¨è§„èŒƒ
- ğŸ’¡ æä¾›æˆæœ¬ä¼˜åŒ–å»ºè®®

---

*è¯¥æ¨¡å—æ˜¯æ•´ä¸ªç³»ç»ŸAIèƒ½åŠ›çš„æ ¸å¿ƒï¼Œå»ºè®®å®šæœŸæ£€æŸ¥APIä½¿ç”¨æƒ…å†µå’Œæˆæœ¬ã€‚*